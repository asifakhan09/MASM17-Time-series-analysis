{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "4492d026",
   "metadata": {},
   "source": [
    "# Computer Exercise 2\n",
    "## Transfer function models and prediction\n",
    "\n",
    "Time Series Analysis  \n",
    "Lund University, Fall 2025\n",
    "\n",
    "In this computer exercise, we will work with input-output relations, as well as prediction in time series models. Firstly, you will be acquainted with time series having an exogenous input, having to analyze the impulse response of such a system and from it build a suitable model. Secondly, we will examine how one can predict a time series, perhaps the most important application of time series modeling. You will be expected to make predictions of all models introduced in this course."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4ccf1f87",
   "metadata": {},
   "source": [
    "## Preparations before the lab\n",
    "\n",
    "Review chapters 3, 4, and carefully read chapter 6 in the course textbook. Make sure to read section 4.5 in particular, as it deals with transfer function models, as well as this entire computer exercise guide.\n",
    "\n",
    "Answers to some of the computer exercise will be graded using the course's *Mozquizto* page. Ensure that you can access the system before the exercise and answer the preparatory questions as well as (at least) three of numbered exercise questions below *before the exercise*.\n",
    "\n",
    "You can find the *Mozquizto* system at <https://quizms.maths.lth.se>\n",
    "\n",
    "It should be stressed that a thorough understanding of the material in this exercise is important to be able to complete the course project, and we encourage you to discuss any questions you might have on the exercises with the teaching staff. This will save you a lot of time when you start working with the project!\n",
    "\n",
    "You are allowed to solve the exercise in groups of two, but not more. Please respect this."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "id": "1d7342ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import necessary libraries\n",
    "%matplotlib qt\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from scipy import signal\n",
    "import sys\n",
    "import os\n",
    "import importlib\n",
    "import scipy.io\n",
    "\n",
    "# Add path to tsa_lth library\n",
    "sys.path.append(os.path.abspath(os.path.join(os.getcwd(), '..', 'TimeSeriesAnalysis-main', 'TimeSeriesAnalysis-main')))\n",
    "\n",
    "from tsa_lth.analysis import plotACFnPACF, normplot, pzmap, kovarians, xcorr\n",
    "from tsa_lth.modelling import estimateARMA, polydiv, estimateBJ, PEM\n",
    "from tsa_lth.tests import whiteness_test, check_if_normal\n",
    "from scipy.stats import norm\n",
    "\n",
    "# Set data directory\n",
    "DATA_DIR = os.path.join(os.getcwd(), '..', 'data')\n",
    "\n",
    "\n",
    "# Function to compute cross correlation, xcorr function had the wrong input order of x and y. Also had a lag shift due to filtering but this works. \n",
    "def compute_ccf(x, y, maxlag):\n",
    "    Cxy = np.correlate(y - np.mean(y), x - np.mean(x), mode='full')\n",
    "    Cxy = Cxy / (np.std(y) * np.std(x) * len(y))\n",
    "    lags = np.arange(-maxlag, maxlag + 1)\n",
    "    mid = len(Cxy) // 2\n",
    "    Cxy = Cxy[mid - maxlag:mid + maxlag + 1]\n",
    "    return lags, Cxy\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6bf82fc7",
   "metadata": {},
   "source": [
    "## Lab Tasks\n",
    "\n",
    "### 2.1 Modeling of an exogenous input signal\n",
    "\n",
    "In this and in the next section, you will work with modeling of input-output relations, both using the ARMAX model and the transfer function model framework. As modeling of a signal which has an exogenous input (an input which is known, i.e., deterministic) is generally more complex than the common time series models encountered so far in this course, one must take care and proceed with caution. Often very simple models of a low order will suffice, while complex ones will only add variance, detrimental to the precision of predictions.\n",
    "\n",
    "We start by creating a typical time series with a deterministic input signal, using a slight generalization of the ARMAX model, i.e., the Box-Jenkins (BJ) model, having the form of\n",
    "\n",
    "$$\n",
    "y_t = \\frac{B(z) z^{-d}}{A_2(z)} x_{t} + \\frac{C_1(z)}{A_1(z)} e_t \n",
    "$$\n",
    "\n",
    "where $y_t$ is the output signal, $e_t$ is a white noise, $x_t$ is the input signal, and $d$ is the time delay between input and output. Note that if $A_1(z) = A_2(z)$, we have the standard ARMAX model.\n",
    "\n",
    "Begin by generating some data following the Box-Jenkins model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "44ebfc2c",
   "metadata": {},
   "outputs": [],
   "source": [
    "rng = np.random.default_rng(0)\n",
    "n = 500 #number of samples\n",
    "\n",
    "# Generate input signal\n",
    "A3 = np.array([1.0, 0.5])\n",
    "C3 = np.array([1.0, -0.3, 0.2])\n",
    "w = np.sqrt(2.0) * rng.standard_normal(n + 100)\n",
    "x = signal.lfilter(C3, A3, w)\n",
    "\n",
    "# Generate output signal\n",
    "A1 = np.array([1.0, -0.65])\n",
    "A2 = np.array([1.0, 0.90, 0.78])\n",
    "B = np.array([0.0, 0.0, 0.0, 0.0, 0.4])\n",
    "C = np.array([1.0])\n",
    "e = np.sqrt(1.5) * rng.standard_normal(n + 100)\n",
    "y = signal.lfilter(C, A1, e) + signal.lfilter(B, A2, x)\n",
    "\n",
    "# Remove samples\n",
    "x = x[100:]\n",
    "y = y[100:]\n",
    "\n",
    "# Clear the true parameters\n",
    "del A1, A2, B, C, e, w, A3, C3"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8a2d10f6",
   "metadata": {},
   "source": [
    "Here, the known input $x_t$ has been generated as an ARMA(1,2) process.\n",
    "\n",
    "**Remark:** As discussed in the first computer exercise, we typically generate more data than needed when simulating a process to avoid initialisation effects. We here also clear the variables used to create the signals to avoid the risk of accidentally referring to these later in the code. One notable benefit of using simulated data in this way is that we know the true values we seek, so we can compare our results with these to see if our code works properly.\n",
    "\n",
    "In order to now model $y_t$ as a time series formed from $x_t$ and $e_t$, several steps must be taken beyond regular ARMA modeling. We must first select the appropriate model orders for the polynomials in the model, then proceeding to estimate the parameters of these polynomials. This may be done in various ways; here, we will follow the steps outlined in Section 4.5 in the course textbook. However, it should be noted that if you can select your model orders in another way, including simply guessing, this is fully acceptable - what counts is if your model actually works, not the intermediate steps used to designed it!\n",
    "\n",
    "#### Step 1: Determine orders of B(z) and A₂(z)\n",
    "\n",
    "As a first step, we wish to determine the orders of the $B(z)$ and $A_2(z)$ polynomials. Using the transfer function framework, we denote the transfer function from $x_t$ to $y_t$ by $H(z) = B(z)z^{-d} / A_2(z)$. In order to estimate the order of the $B(z)$ and $A_2(z)$ polynomials, as well as determining the delay $d$, we need to form an estimate of the (possibly infinite) impulse response, and from it identify the appropriate models for these polynomials.\n",
    "\n",
    "As noted in the course textbook, if $x_t$ is a white noise, the (scaled) impulse response can be directly estimated using the cross correlation function (CCF) from $x_t$ to $y_t$. However, if $x_t$ is not white, we need to perform pre-whitening, i.e., we need to form a model for the input, such that it may be viewed as being driven by a white noise, and then inverse filter both input and output with this model. In order to do so, we form an ARMA model of the input\n",
    "\n",
    "$$\n",
    "A_3(z) x_t = C_3(z) w_t\n",
    "$$\n",
    "\n",
    "and then replace $x_t$ with $w_t$, i.e.,\n",
    "\n",
    "$$\n",
    "y_t =  \\frac{B(z)z^{-d}}{A_2(z)} \\frac{C_3(z)}{A_3(z)} w_t + \\frac{C_1(z)}{A_1(z)} e_t \n",
    "$$\n",
    "\n",
    "The pre-whitening step, i.e., multiplying with $A_3(z) / C_3(z)$, yields\n",
    "\n",
    "$$\n",
    "\\underbrace{\\frac{A_3(z)}{C_3(z)} y_t}_{\\epsilon_t} =  \\underbrace{\\frac{B(z)z^{-d}}{A_2(z)}}_{H(z)}  w_t +  \\underbrace{\\frac{A_3(z)}{C_3(z)} \\frac{C_1(z)}{A_1(z)} e_t}_{v_t} \n",
    "$$\n",
    "\n",
    "and the preferred transfer function model may thus be expressed as\n",
    "\n",
    "$$\n",
    "\\epsilon_t=   H(z) w_t + v_t \n",
    "$$\n",
    "\n",
    "Note that the pre-whitened $\\epsilon_t$ is now the output of the transfer function model, having the preferred uncorrelated signal as its input, allowing $H(z)$ to be estimated using the CCF from $w_t$ to $\\epsilon_t$.\n",
    "\n",
    "**Task:** Use the basic analysis (acf, pacf, and normplot) to create an ARMA model for the input signal $x_t$ as a function of a white noise, $w_t$. Which model did you find most suitable for $x_t$? Is it reasonably close to the one you used to generate the input?\n",
    "\n",
    "**QUESTION 1:** In Mozquizto, answer question 1."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "id": "058df43f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Whiteness test with 5.0% significance\n",
      "  Ljung-Box-Pierce test: True (white if 20.58 < 37.65)\n",
      "  McLeod-Li test:        True (white if 28.96 < 37.65)\n",
      "  Monti test:            True (white if 20.97 < 37.65)\n",
      "  Sign change test:      True (white if 0.50 in [0.46,0.54])\n",
      "Discrete-time AR model: A(z)y(t) = e(t)\n",
      "\n",
      "A(z) = 1.0 + 0.8529(±0.0442)·z⁻¹ + 0.1571(±0.0442)·z⁻²\n",
      "\n",
      "Polynomial orders: nA = 2\n",
      "Number of free coefficients: 2\n",
      "Fit to estimation data (NRMSE): 33.14%\n",
      "FPE : 2.034  MSE : 2.023\n",
      "AIC : 1768.243   BIC : 1776.664\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Analyze input signal x and estimate its ARMA model. Analyze residuals and create w_t and eps_t, \n",
    "\n",
    "# Plot x to get a feel of the data. \n",
    "fig, ax = plt.subplots(figsize=(12, 8))\n",
    "ax.plot(x)\n",
    "ax.set_title('Process x')\n",
    "ax.set_xlabel('Sample')\n",
    "ax.set_ylabel('x')\n",
    "ax.grid(True, alpha=0.3)\n",
    "# The data already looks stationary. \n",
    "\n",
    "# Plot ACF and PACF to start estimate model order \n",
    "plotACFnPACF(x, noLags=40, signLvl=0.05)\n",
    "# Directly plotting them for the data indicates possibly 2 AR components to start with \n",
    "\n",
    "# Keep numbering as book/instructions \n",
    "p3 = 2\n",
    "q3 = 0\n",
    "# Estimate model for the input \n",
    "inp_mod = estimateARMA(x, A = p3, C = q3, plot=False)\n",
    "w_t = signal.lfilter(inp_mod.A, inp_mod.C, x)[p3:]\n",
    "eps_t = signal.lfilter(inp_mod.A, inp_mod.C, y)[p3:]\n",
    "\n",
    "# Plot ACF, PACF And Normplot \n",
    "plotACFnPACF(w_t, noLags=40, signLvl=0.05)\n",
    "plt.figure()\n",
    "normplot(w_t)\n",
    "whiteness_test(w_t)\n",
    "inp_mod.summary()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "73652388-aa55-4097-9af3-d7393ce654df",
   "metadata": {},
   "source": [
    "## Conclusions from modelling input \n",
    "\n",
    "### Test 1: AR(2) \n",
    "\n",
    "Based on the initial ACF and PACF, an AR(2) was tested. This resulted in white residuals, including  \n",
    "monti test and normal distributions according to normplot. Some key values are,  \n",
    "$A_3(z) = 1 + 0.8529(\\pm 0.0442)z^{-1} + 0.1571(\\pm 0.0442)z^{-2}$  \n",
    "Monti test quantity: $20.97 < 37.65$  \n",
    "$\\mathrm{FPE} = 2.034$  \n",
    "$\\mathrm{BIC} = 1776.7$  \n",
    "This could be a good candidate for a model. Would it be possible to go simpler? \n",
    "\n",
    "\n",
    "### Test 2: AR(1)\n",
    "\n",
    "We're testing if we can get away with an even simpler model, and we almost can, but there are a few  \n",
    "more residuals slightly over the confidence interval. Still normal residuals, thus we trust the  \n",
    "whiteness tests.  \n",
    "$A_3(z) = 1 + 0.7365(\\pm 0.0302)z^{-1}$  \n",
    "Monti test quantity: $33.87 < 37.65$  \n",
    "$\\mathrm{FPE} = 2.077$  \n",
    "$\\mathrm{BIC} = 1786.1$  \n",
    "It is slightly worse on FPE and BIC. The Monti test is also closer to the confidence limit. This model  \n",
    "is not quite as good as the AR(2)  \n",
    "\n",
    "\n",
    "### Test 3: ARMA(1,1)  \n",
    "\n",
    "Since the AR(1) was pretty good, what happens if we create an ARMA model?  \n",
    "$A_3(z) = 1 + 0.6616(\\pm 0.0449)z^{-1}$  \n",
    "$C_3(z) = 1 - 0.1698(\\pm 0.0590)z^{-1}$  \n",
    "Monti test quantity:  $23.31 < 37.65$  \n",
    "$\\mathrm{FPE} = 2.043$  \n",
    "$\\mathrm{BIC} = 1782.3$  \n",
    "Very similar to the AR(2), but the BIC is slightly worse. \n",
    "\n",
    "\n",
    "### Test 4: ARMA(1, 2)\n",
    "\n",
    "We know whis is the correct model, however, would we select it based on the data if we did not have this knowledge?  \n",
    "$A_3(z) = 1 + 0.5438(\\pm 0.0735) z^{-1}$  \n",
    "$C_3(z) = 1 - 0.2994(\\pm 0.0807) z^{-1} + 0.1598(\\pm 0.0651)z^{-2}$  \n",
    "Monti test quantity: $18.10 < 37.65$  \n",
    "$\\mathrm{FPE} = 2.030$  \n",
    "$\\mathrm{BIC} = 1283.1$\n",
    "We see that, yes the Monti test quantity is lower than the AR(2), but the FPE is only marginally better and the BIC is worse.  \n",
    "If I had this data which model would I choose? Probably keep it simple and use the AR(2).  \n",
    "\n",
    "Testing and ARMA(2,2) does not result in an improvement, and the $a_2$ coefficient is unsurprisingly not significant.  \n",
    "\n",
    "Conclusion: If we did not have knowledge about the process, an AR(2) would be a reasonable choice. \n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f2e1eed3",
   "metadata": {},
   "source": [
    "We then pre-whiten $y_t$, creating $\\epsilon_t$. Next, we compute the CCF from $w_t$ to $\\epsilon_t$. It should be stressed that we *only* use the pre-whitened signals to form this CCF. These signals are then not used in any of the remaining steps."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "id": "a92a36d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute and plot cross-correlation between w_t and eps_t\n",
    "\n",
    "lags, ccf_vals = compute_ccf(w_t, eps_t, maxlag=40)\n",
    "fig, ax = plt.subplots()\n",
    "ax.stem(lags, ccf_vals, basefmt=' ')\n",
    "condInt = 2 / np.sqrt(len(w_t))\n",
    "ax.axhline(condInt, color='r', linestyle='--', label='95% confidence')\n",
    "ax.axhline(-condInt, color='r', linestyle='--')\n",
    "ax.set_xlabel('Lag')\n",
    "ax.set_ylabel('CCF')\n",
    "ax.set_title('Cross-correlation between w_t and eps_t')\n",
    "plt.grid(True)\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "da403c2e-ef7d-414f-9f65-d9a41843a762",
   "metadata": {},
   "source": [
    "## Conclusions \n",
    "\n",
    "It's is easy to see that the delay is probably $4$, however, the remaining parameters are not so clear.  \n",
    "We can hope that anything more than lalg 4 from the delay is just due to the previous terms. I'm not sure  \n",
    "this would be classified as ringing, most samples are just in the confidence intervals, but negative.  \n",
    "It is difficult to tell if there is an AR part. There is most likely an MA part, possible with $s = 3$?  \n",
    "Lets try that, ie:  \n",
    "$d = 4$  \n",
    "$r = 0$  \n",
    "$s = 3$  \n",
    "\n",
    "However, we know this is not actually correct. it should be $r = 2$, $s = 0$.  \n",
    "Let's try what it looks like first. \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e80c5eae",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "As the estimated CCF now yields an estimate of the impulse response, $H(z)$, we can proceed to use this to determine suitable model orders for the delay, and the $B(z)$ and $A_2(z)$ polynomials using Table 4.7 in the textbook. Use `PEM` ( one can also use the`estimateBJ`) to estimate your model, where the delay may be added to `B` by adding $d$ zeros in the beginning of the vector. If the model orders are suitable, the CCF between the input, $x_t$, and the residual $\\tilde{e}_t$ (defined below) should be uncorrelated.\n",
    "\n",
    "**Task:** Analyze the CCF of $w_t$ to $\\epsilon_t$ to find the model orders of the transfer function. Calculate the residual $\\tilde{e}_t$ and verify that it is uncorrelated with $x_t$. Also, analyze the residual using the regular basic analysis. Can you conclude that $\\tilde{e}_t$ is white noise? Should it be?\n",
    "\n",
    "**QUESTION 2:** In Mozquizto, answer question 2."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "id": "abd30f58",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Discrete-time BJ model: y(t) = [B(z)/F(z)]x(t) + e(t)\n",
      "\n",
      "B(z) = 0.4038(±0.0222)·z⁻⁴\n",
      "F(z) = 1.0 + 0.899(±0.0207)·z⁻¹ + 0.7762(±0.0204)·z⁻²\n",
      "\n",
      "Polynomial orders: nB = 4    nF = 2\n",
      "Number of free coefficients: 3\n",
      "Fit to estimation data (NRMSE): 27.37%\n",
      "FPE : 2.705  MSE : 2.703\n",
      "AIC : 1906.818   BIC : 1919.437\n",
      "\n"
     ]
    }
   ],
   "source": [
    "B_init = np.array([0, 0, 0, 0, 1])\n",
    "A2_init = np.array([1, 0, 0])\n",
    "C1_init = np.array([1])\n",
    "A1_init = np.array([1])\n",
    "\n",
    "model_ba2 = PEM(y, x, B=B_init, F=A2_init, C=C1_init, D=A1_init)\n",
    "B_free = np.array([0, 0, 0, 0, 1])\n",
    "A2_free = np.array([1, 1, 1])\n",
    "C1_free = np.array([1])\n",
    "A1_free = np.array([1])\n",
    "model_ba2.set_free_params(B_free=B_free, F_free=A2_free, C_free=C1_free, D_free=A1_free)\n",
    "Mba2 = model_ba2.fit()\n",
    "etilde = Mba2.resid\n",
    "Mba2.summary()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cd5b5283-0c23-492f-bbb0-2c838e963c36",
   "metadata": {},
   "source": [
    "## Attempt 1: \n",
    "we get: \n",
    "$B(z) = 0.408z^{-1} - 0.3848z^{-5} - 0.278z^{-6}$    (All coefficients significant)  \n",
    "$\\mathrm{FPE} = 3.539$  \n",
    "$\\mathrm{BIC} = 2043.6$  \n",
    "Let's continue investigating and see how this holds up when we consider the full model  \n",
    "I am unsure how well the below figure corresponds to this. \n",
    "\n",
    "## Attempt 2: \n",
    "Assume we instead were able to correctly read the model parameters, $d = 4$, $r = 2$, $s = 0$, we get:  \n",
    "$B(z) = 0.4038 z^{-4}$  \n",
    "$A_2(z) = 1 + 0.899z^{-1} + 0.7762z^{-2}$ (All parameters significant)  \n",
    "$\\mathrm{FPE} = 2.705$  \n",
    "$\\mathrm{BIC} = 1919.4$  \n",
    "\n",
    "\n",
    "But does the FPE etc refer to the full model, since we use the same function later, surely it must? \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "291e9ca5",
   "metadata": {},
   "source": [
    "#### Step 2: Check the input contribution\n",
    "\n",
    "It is always wise to examine how much of the output signal that is described by the input signal. To examine this, plot the output as compared to the filtered input. Note that we, as usual, need to remove the corrupt samples from the filtered input, and thus also from `y` to keep the signals in sync.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "id": "03be93d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "xfilt = signal.lfilter(Mba2.B, Mba2.F, x)\n",
    "y_cut = y[len(Mba2.F):]\n",
    "xfilt_cut = xfilt[len(Mba2.F):]\n",
    "\n",
    "fig, ax = plt.subplots()\n",
    "ax.plot(y_cut, label='Output y', alpha=0.7 )\n",
    "ax.plot(xfilt_cut, label='Filtered input (B/A2)x', alpha=0.7)\n",
    "ax.legend()\n",
    "ax.grid(True)\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "61da118d",
   "metadata": {},
   "source": [
    "Clearly, these two signals will rarely be the same (or even close to the same), but you want to see that the (filtered) input is indeed describing a significant part of the output - and that it is in phase with the output, so that when you subtract the two (below), the residual ($\\tilde{e}_t$) becomes \"smaller\" than the original output. It should be stressed that our model is not yet completed, so the here used polynomials will not be in their final form - but as it can happen that one \"loses the input\", i.e., the input becomes less important, when one proceed with the modelling, it is wise to check this part already now - and then do so again when the model is complete, to ensure that one still use the input properly. If this is not the case, you are creating a problem and need to redo the first steps..."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3d0857a2",
   "metadata": {},
   "source": [
    "## #### Step 3: Model the ARMA part\n",
    "\n",
    "We have now modeled $y_t$ as a function of the input $x_t$, but have not yet formed a model of the ARMA-process in the BJ model, i.e., modeled the polynomials $C_1(z)$ and $A_1(z)$. Therefore, defining the ARMA-part as\n",
    "\n",
    "$$\n",
    "\\tilde{e}_t = \\frac{C_1(z)}{A_1(z)} e_t\n",
    "$$\n",
    "\n",
    "we use the estimated polynomials $B(z)$ and $A_2(z)$ and estimate $\\tilde{e}_t$ as\n",
    "\n",
    "$$\n",
    "\\tilde{e}_t = y_t -  \\frac{\\hat{B}(z)z^{-\\hat{d}}}{\\hat{A}_2(z)} x_t\n",
    "$$\n",
    "\n",
    "By filtering out the input-dependent part of the process $y_t$, we may then determine suitable orders for the polynomials $C_1(z)$ and $A_1(z)$ using the standard ARMA-modeling procedure.\n",
    "\n",
    "**Task:** Use the estimates of the polynomials $B(z)$ and $A_2(z)$ obtained for the pre-whitened data, plot the filtered input as compared to the output, and form $\\tilde{e}_t$. Determine suitable model orders for $A_1(z)$ and $C_1(z)$. Was all dependence from $x_t$ removed in $\\tilde{e}_t$? (For real data, there is often remaining dependencies in the data - this should not come as a surprise given the simplistic models we use. Do not let this worry you, rather proceed to examine if the model works. If it does, then this is likely nothing to worry about...)\n",
    "\n",
    "**QUESTION 3:** In Mozquizto, answer question 3."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "id": "9751175d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Whiteness test with 5.0% significance\n",
      "  Ljung-Box-Pierce test: True (white if 19.68 < 37.65)\n",
      "  McLeod-Li test:        False (white if 40.77 < 37.65)\n",
      "  Monti test:            True (white if 18.79 < 37.65)\n",
      "  Sign change test:      True (white if 0.49 in [0.46,0.54])\n",
      "Discrete-time AR model: A(z)y(t) = e(t)\n",
      "\n",
      "A(z) = 1.0 + 0.8529(±0.0442)·z⁻¹ + 0.1571(±0.0442)·z⁻²\n",
      "\n",
      "Polynomial orders: nA = 2\n",
      "Number of free coefficients: 2\n",
      "Fit to estimation data (NRMSE): 33.14%\n",
      "FPE : 2.034  MSE : 2.023\n",
      "AIC : 1768.243   BIC : 1776.664\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# analyze the etilde and estimate order for A1, C1 \n",
    "\n",
    "# Plot etilde \n",
    "fig, ax = plt.subplots()\n",
    "ax.plot(etilde)\n",
    "ax.set_xlabel('sample')\n",
    "ax.set_ylabel('$\\\\tilde{e}_t$')\n",
    "fig.suptitle('Plotted data for $\\\\tilde{e}_t$')\n",
    "\n",
    "# Plot ACF and PACF to see if this can be modeled as an ARMA \n",
    "plotACFnPACF(etilde)\n",
    "\n",
    "# Hard to tell what it should be... Let's try some models \n",
    "# Keep numbering as book/instructions \n",
    "p1 = 1\n",
    "q1 = 0\n",
    "# Estimate model for the input \n",
    "model = estimateARMA(etilde, A = p1, C = q1, plot=False)\n",
    "et = signal.lfilter(model.A, model.C, etilde)[p1:]\n",
    "\n",
    "# Plot ACF, PACF And Normplot \n",
    "plotACFnPACF(et, noLags=40, signLvl=0.05)\n",
    "plt.figure()\n",
    "normplot(et)\n",
    "whiteness_test(et)\n",
    "inp_mod.summary()\n",
    "\n",
    "# Clear third component in PACF \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a48fd210-aeaf-418e-94cf-a35a4fc71cac",
   "metadata": {},
   "source": [
    "## Finalizing \n",
    "\n",
    "First, we test given the model with $s=3$. This should possibly not work. Do we detect this?  \n",
    "We now see a large PACF contribution for lag 3, Possibly we can include this one and skip lag 2. Do we need an MA component?  \n",
    "As there are still some contrinutions outside the confidence bands, this may be the case. An ARMA(3,1) looks better, possibly  \n",
    "with $a_2 = 0$ "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "120797dc",
   "metadata": {},
   "source": [
    "#### Step 4: Estimate complete BJ model\n",
    "\n",
    "Finally, now having determined all the polynomial orders in our model, we estimate all polynomials all together using the estimation function. Here, `ehat` is the estimate of the noise process e_t; notice that this is not the same process as $\\tilde{e}_t$ (which is the filtered version of e_t as shown above). The intermediate residual from the previous steps is stored in the variable `etilde`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 155,
   "id": "48d65cf9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Discrete-time BJ model: y(t) = [B(z)/F(z)]x(t) + [1/D(z)]e(t)\n",
      "\n",
      "B(z) = 0.4019(±0.0101)·z⁻⁴\n",
      "D(z) = 1.0 - 0.7068(±0.0317)·z⁻¹\n",
      "F(z) = 1.0 + 0.8998(±0.0099)·z⁻¹ + 0.7771(±0.0095)·z⁻²\n",
      "\n",
      "Polynomial orders: nB = 4    nD = 1    nF = 2\n",
      "Number of free coefficients: 4\n",
      "Fit to estimation data (NRMSE): 48.68%\n",
      "FPE : 1.369  MSE : 1.35\n",
      "AIC : 1564.291   BIC : 1581.118\n",
      "\n"
     ]
    }
   ],
   "source": [
    "B_init = np.array([0, 0, 0, 0, 1])\n",
    "A2_init = np.array([1, 0, 0])\n",
    "C1_init = np.array([1])\n",
    "A1_init = np.array([1, 0])\n",
    "#B_init = np.array([0, 0, 0, 0, 1, 0, 0])\n",
    "#A2_init = np.array([1])\n",
    "#C1_init = np.array([1, 0])\n",
    "#A1_init = np.array([1, 0, 0, 0])\n",
    "\n",
    "model_boxj = PEM(y, x, B=B_init, F=A2_init, C=C1_init, D=A1_init)\n",
    "B_free = np.array([0, 0, 0, 0, 1])\n",
    "A2_free = np.array([1, 1, 1])\n",
    "C1_free = np.array([1])\n",
    "A1_free = np.array([1, 1])\n",
    "#B_free = np.array([0, 0, 0, 0, 1, 1, 1])\n",
    "#A2_free = np.array([1])\n",
    "#C1_free = np.array([1, 1])\n",
    "#A1_free = np.array([1, 1, 0, 1])\n",
    "model_boxj.set_free_params(B_free=B_free, F_free=A2_free, C_free=C1_free, D_free=A1_free)\n",
    "MboxJ = model_boxj.fit()\n",
    "ehat = MboxJ.resid\n",
    "MboxJ.summary()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "82ce6b06-e269-4a89-8993-107eff717015",
   "metadata": {},
   "source": [
    "## Comments\n",
    "\n",
    "### Model 1\n",
    "\n",
    "This model results in  \n",
    "$B(z) = 0.3961z^{-4} - 0.4629z^{-5} - 0.4288z^{-6}$  \n",
    "$C_1(z) = 1 + 0.3753z^{-1}$  \n",
    "$A_1(z) = 1 - 0.1155z^{-1} - 0.4871z^{-3}$  \n",
    "All coefficient are significant  \n",
    "$\\mathrm{NRMSE} = 36.51 \\%$    \n",
    "$\\mathrm{FPE} = 2.114$  \n",
    "$\\mathrm{BIC} = 1797.6$  \n",
    "\n",
    "### Model 2\n",
    "\n",
    "With the correct estimation of $B(z)$ and $A_2(z)$, assuming we were able to do this, we were first of all able to  \n",
    "estimate a much simpler model for $A_1(z)$ and $C_1(z)$. We found that This process was now described by a simple AR(1).  \n",
    "Re-estimating all parameters, we find that:  \n",
    "$B(z) = 0.4019z^{-4}$  \n",
    "$A_2(z) = 1 + 0.8998z^{-1} + 0.7771z^{-2}$  \n",
    "$A_1(z) = 1 - 0.7068z^{-1}$  \n",
    "$\\mathrm{NRMSE} = 28.68 \\%$    \n",
    "$\\mathrm{FPE} = 1.369$  \n",
    "$\\mathrm{BIC} = 1581.1$  \n",
    "This is unsurprisingly much better than the first attempt. This indicates that it may be worth trying some various attempts at \n",
    "$B(z)$and $A_2(z)$, if it is hard to tell which one is correct.  \n",
    "\n",
    "In addition, we here see that the monti test passes and residuals look normal, This is ont the other hand also correct in the other case,  \n",
    "but the residuals and x may not be completely uncorrelated "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "80e32082",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "#### Step 5: Final verification\n",
    "\n",
    "Check again so that you are still using the input properly by forming the plot in step 2 above. Have you \"lost the input\" as compared to before?\n",
    "\n",
    "**Task:** Are the parameter estimates significantly different from zero? Can you conclude that the residual is white noise, uncorrelated with the input signal? If not, can you twiddle with the model slightly to improve the residual?\n",
    "\n",
    "**Be prepared to answer these questions when discussing with the examiner at the computer exercise!**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 156,
   "id": "c44a75d6-57e6-48f3-89dd-4977444e04a7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Whiteness test with 5.0% significance\n",
      "  Ljung-Box-Pierce test: True (white if 19.80 < 37.65)\n",
      "  McLeod-Li test:        False (white if 40.81 < 37.65)\n",
      "  Monti test:            True (white if 18.61 < 37.65)\n",
      "  Sign change test:      True (white if 0.49 in [0.46,0.54])\n"
     ]
    }
   ],
   "source": [
    "## Analyze the model\n",
    "xfilt_final = signal.lfilter(MboxJ.B, MboxJ.F, x)\n",
    "y_cut_final = y[len(MboxJ.F):]\n",
    "xfilt_cut_final = xfilt[len(MboxJ.F):]\n",
    "\n",
    "fig, ax = plt.subplots()\n",
    "ax.plot(y_cut_final, label='Output y', alpha=0.7 )\n",
    "ax.plot(xfilt_cut_final, label='Filtered input (B/A2)x', alpha=0.7)\n",
    "ax.legend()\n",
    "ax.grid(True)\n",
    "plt.show()\n",
    "\n",
    "# Are the residuals uncorrelated with the input? \n",
    "# Is this even correct???? \n",
    "lags, corr = compute_ccf(ehat, x, 40)\n",
    "fig, ax = plt.subplots() \n",
    "ax.plot(lags, corr)\n",
    "xran = np.array([np.min(lags), np.max(lags)])\n",
    "conf = 2 / np.sqrt(len(ehat)) * np.array([1, 1])\n",
    "ax.plot(xran, conf, '--r')\n",
    "ax.plot(xran, -conf, '--r')\n",
    "\n",
    "# Normplot for residuals \n",
    "plt.figure() \n",
    "normplot(ehat)\n",
    "\n",
    "# Whiteness tests \n",
    "whiteness_test(ehat)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f1e57eca",
   "metadata": {},
   "source": [
    "### 2.2 Prediction of ARMA-processes\n",
    "\n",
    "In this section, we examine how to predict future values of a process, using temperature measurements from the Swedish city Svedala. The temperature data is sampled every hour during a period in April and May 1994, with its (estimated) mean value subtracted (11.35°C).\n",
    "\n",
    "Load the measurements ` svedala`. Suitable model parameters for the data set are:\n",
    "\n",
    "```python\n",
    "A = [ 1, -1.79, 0.84 ]\n",
    "C = [ 1, -0.18, -0.11 ]\n",
    "```\n",
    "\n",
    "To make a $k$-step prediction, $\\hat{y}_{t+k \\mid t}$, one needs to solve the equation\n",
    "\n",
    "$$\n",
    "C(z) \\hat{y}_{t+k \\mid t}=G_k(z)y_t \n",
    "$$\n",
    "\n",
    "This can be done using the filter command (remember to remove the initial samples after using the command `signal.lfilter`):\n",
    "\n",
    "```python\n",
    "yhat_k = signal.lfilter(Gk, C, y)\n",
    "```\n",
    "\n",
    "where $G_k$ is obtained from the Diophantine equation\n",
    "\n",
    "$$\n",
    "C(z)=A(z)F_k(z)+z^{-k}G_k(z).\n",
    "$$\n",
    "\n",
    "Here, we have included the desired prediction range, $k$, in the polynomials $G_k(z)$ and $F_k(z)$ to stress that you will need a different polynomial for each $k$. Thus, if you wish to predict two steps ahead into the future, forming both $\\hat{y}_{t+1 | t}$ and $\\hat{y}_{t+2 | t}$, you will need to use both $G_1(z)$ and $G_2(z)$ to construct these estimates.\n",
    "\n",
    "To solve the Diophantine equation, you can use the provided function `polydiv`:\n",
    "\n",
    "```python\n",
    "[Fk, Gk] = polydiv(C, A, k)\n",
    "```\n",
    "\n",
    "The prediction error is formed as\n",
    "\n",
    "$$\n",
    "y_{t+k}-\\hat{y}_{t+k \\mid t} = F_k(z) e_{t+k},\n",
    "$$\n",
    "\n",
    "Note in particular that the prediction error will (for a perfect model) have the form of an MA($k-1$) process with the generating polynomial\n",
    "\n",
    "$$\n",
    "F_k(z)=1+f_1z^{-1} + \\cdots +f_{k-1}z^{-(k-1)}.\n",
    "$$\n",
    "\n",
    "Note also that if $k=1$, then $F_1(z)=1$, suggesting that the prediction error should be a white noise, and that, for this case, the prediction error thus allows for an estimate of the noise variance.\n",
    "\n",
    "**QUESTION 4:** In Mozquizto, answer question 4.\n",
    "\n",
    "**Task:** In the following questions, examine the $k$-step prediction using $k=3$ and $k=26$. Answer the following questions:\n",
    "\n",
    "1. What is the estimated mean and the expectation of the prediction error for each of these cases?\n",
    "2. Assuming that the estimated noise variance is the true one, what is the theoretical variance of the prediction error? Using the same noise variance, what is the estimated variance of the prediction error? Comment on the differences in these variances.\n",
    "3. For each of the cases, determine the theoretical 95% confidence interval of the prediction errors?\n",
    "4. How large percentage of the prediction errors are outside the 95% confidence interval? A useful trick might be to use `sum(res>c)` to compute how many elements in `res` that are greater than `c`.\n",
    "5. Plot the process and the predictions in the same plot, and in a separate figure, plot the residuals. Check if the sequence of residuals behaves as an MA($k-1$) process by, e.g., estimating its covariance function using `covf`. If it does not, is it close?\n",
    "\n",
    "**Be prepared to answer these questions when discussing with the examiner at the computer exercise!**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 191,
   "id": "b60ee4ff",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prediction error: 0.007684794680036222, Estimated noise variance: 0.37485026059692883\n",
      "The confidence intervals is +/- 3.313928099285431\n",
      "Percent errors outside confidence interval: 5.813097866077999 %\n"
     ]
    }
   ],
   "source": [
    "# Load svedala temperature data\n",
    "mat = scipy.io.loadmat(os.path.join(DATA_DIR, 'svedala.mat'))\n",
    "svedala = mat['svedala'].flatten()\n",
    "\n",
    "# Create a model the data and do predictions\n",
    "k = 3\n",
    "A = [ 1, -1.79, 0.84 ]\n",
    "C = [ 1, -0.18, -0.11 ]\n",
    "\n",
    "[Fk, Gk] = polydiv(C, A, k)\n",
    "\n",
    "yhat = signal.lfilter(Gk, C, svedala)\n",
    "\n",
    "yhat_rem = yhat[2:]\n",
    "svedala_rem = svedala[2:]\n",
    "\n",
    "fig, ax = plt.subplots()\n",
    "ax.plot(svedala_rem)\n",
    "ax.plot(yhat_rem, 'r-')\n",
    "ax.set_xlabel('sample')\n",
    "ax.set_ylabel('$y_t$')\n",
    "fig.suptitle(f'Data and prediction, $k = {k}$')\n",
    "\n",
    "# Calculate mean prediction error \n",
    "pred_errs = yhat_rem - svedala_rem\n",
    "mpe = np.sum(pred_errs) / len(yhat_rem) \n",
    "\n",
    "# Filter process to get estimated e \n",
    "ehat = signal.lfilter(A, C, svedala)[2:]\n",
    "s2hat = np.sum(ehat**2) / len(ehat)\n",
    "\n",
    "print(f\"Prediction error: {mpe}, Estimated noise variance: {s2hat}\")\n",
    "\n",
    "# Estimate prediction variance \n",
    "predvar = np.sum(Fk ** 2) * s2hat\n",
    "confint = 2 * np.sqrt(predvar)\n",
    "\n",
    "print(f\"The confidence intervals is +/- {confint}\")\n",
    "\n",
    "# Add to plot \n",
    "#ax.plot(yhat_rem + confint, 'r--')\n",
    "# ax.plot(yhat_rem - confint, 'r--')\n",
    "\n",
    "percent_outside = 100 * np.sum(np.abs(pred_errs) > confint) / len(pred_errs)\n",
    "print(f\"Percent errors outside confidence interval: {percent_outside} %\")\n",
    "\n",
    "# New plot for residuals \n",
    "fig, ax = plt.subplots()\n",
    "ax.plot(pred_errs)\n",
    "\n",
    "# Where is the covf function saved? I cannot find it? \n",
    "# However, plotting the ACF, it seems reasonable that it is an MA(2)\n",
    "plotACFnPACF(pred_errs)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "39b7db95-9b2f-47c2-8b01-5ad21e68ee4a",
   "metadata": {},
   "source": [
    "## Answers  \n",
    "\n",
    "The mean of the prediction error is 0.001, it should be 0. Thus it is close.   \n",
    "Estimated variance. 0.3748, then the prediction variance is $(1 + f_1^2 + f_2^2) \\hat{\\sigma}_e^2$  \n",
    "The confidence interval based on the estiamted variance for the $k=3$ case is, +/- 3.3139  \n",
    "$5.81 \\%$ of prediction errors are outside this interval. Reasonable, considering it is an estimate.   \n",
    "\n",
    "$k=26$ case below\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 192,
   "id": "709a3183-56fa-450b-ab33-addd61768524",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prediction error: 0.005335631757784207, Estimated noise variance: 0.37485026059692883\n",
      "The confidence intervals is +/- 7.101735322784891\n",
      "Percent errors outside confidence interval: 2.9433406916850626 %\n"
     ]
    }
   ],
   "source": [
    "# Load svedala temperature data\n",
    "mat = scipy.io.loadmat(os.path.join(DATA_DIR, 'svedala.mat'))\n",
    "svedala = mat['svedala'].flatten()\n",
    "\n",
    "# Create a model the data and do predictions\n",
    "k = 26\n",
    "A = [ 1, -1.79, 0.84 ]\n",
    "C = [ 1, -0.18, -0.11 ]\n",
    "\n",
    "[Fk, Gk] = polydiv(C, A, k)\n",
    "\n",
    "yhat = signal.lfilter(Gk, C, svedala)\n",
    "\n",
    "yhat_rem = yhat[2:]\n",
    "svedala_rem = svedala[2:]\n",
    "\n",
    "fig, ax = plt.subplots()\n",
    "ax.plot(svedala_rem)\n",
    "ax.plot(yhat_rem, 'r-')\n",
    "ax.set_xlabel('sample')\n",
    "ax.set_ylabel('$y_t$')\n",
    "fig.suptitle(f'Data and prediction, $k = {k}$')\n",
    "\n",
    "# Calculate mean prediction error \n",
    "pred_errs = yhat_rem - svedala_rem\n",
    "mpe = np.sum(pred_errs) / len(yhat_rem) \n",
    "\n",
    "# Filter process to get estimated e \n",
    "ehat = signal.lfilter(A, C, svedala)[2:]\n",
    "s2hat = np.sum(ehat**2) / len(ehat)\n",
    "\n",
    "print(f\"Prediction error: {mpe}, Estimated noise variance: {s2hat}\")\n",
    "\n",
    "# Estimate prediction variance \n",
    "predvar = np.sum(Fk ** 2) * s2hat\n",
    "confint = 2 * np.sqrt(predvar)\n",
    "\n",
    "print(f\"The confidence intervals is +/- {confint}\")\n",
    "\n",
    "# Add to plot \n",
    "#ax.plot(yhat_rem + confint, 'r--')\n",
    "# ax.plot(yhat_rem - confint, 'r--')\n",
    "\n",
    "percent_outside = 100 * np.sum(np.abs(pred_errs) > confint) / len(pred_errs)\n",
    "print(f\"Percent errors outside confidence interval: {percent_outside} %\")\n",
    "\n",
    "# New plot for residuals \n",
    "fig, ax = plt.subplots()\n",
    "ax.plot(pred_errs)\n",
    "\n",
    "# Where is the covf function saved? I cannot find it? \n",
    "# However, plotting the ACF, it seems reasonable that it is an MA(2)\n",
    "plotACFnPACF(pred_errs)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b545cba9",
   "metadata": {},
   "source": [
    "### 2.3 Prediction of ARMAX-processes\n",
    "\n",
    "When predicting ARMAX-processes, one needs to consider also the external input. We will now make use of an additional temperature measurement done at the airport Sturup. The Swedish Meteorological and Hydrological Institute (SMHI) has made a 3-step predictions of the temperature for Sturup, which may be used as an external input signal to our temperature measurements in Svedala. (The provided signals are in sync, so the value at time $t$ in Sturup is the predicted value corresponding to that time. Thus, you do not need to shift the input.)\n",
    "\n",
    "Load the SMHI predictions `sturup`, and set the model parameters to be:\n",
    "\n",
    "```python\n",
    "A = [ 1, -1.49, 0.57 ]\n",
    "B = [ 0, 0, 0, 0.28, -0.26 ]\n",
    "C = [ 1 ]\n",
    "```\n",
    "\n",
    "How large is the delay in this temperature model? How do you know?\n",
    "\n",
    "Form the $k$-step predictor of the temperature at Svedala using the Svedala predictions as input using\n",
    "\n",
    "$$\n",
    "C(z)\\hat{y}_{t+k \\mid t} = B(z)F_k(z)x_t+G_k(z)y_t,\n",
    "$$\n",
    "\n",
    "where $F_k(z)$ and $G_k(z)$ are computed as indicated above. (You should thus not construct a model for the input or output in this example, but instead just use the given polynomials.) The $k$-step prediction is then formed as\n",
    "\n",
    "$$\n",
    "\\hat{y}_{t+k | t} = \\hat{F}_k(z) \\hat{x}_{t+k|t} + \\frac{\\hat{G}_k(z)}{C(z)} x_t + \\frac{G_k(z)}{C(z)} y_t \n",
    "$$\n",
    "\n",
    "where $\\hat{x}_{t+k}$ denotes the predicted future inputs, and the polynomials $\\hat{F}_k(z)$ and $\\hat{G}_k(z)$ are given by the Diophantine equation\n",
    "\n",
    "$$\n",
    "B(z)F_k(z)=C(z) \\hat{F}_k(z) + z^{-k}\\hat{G}_k(z)\n",
    "$$\n",
    "\n",
    "In the prediction, the two first terms represents the contribution of the input signal, with the first term being the prediction of the input signal (in this example, this is thus the prediction of the \"predicted temperature\"; perhaps better not to think about this too much :-)), whereas the third term is from the ARMA part of the process. Form predictions for both $k=3$ and $k=26$.\n",
    "\n",
    "**QUESTION 5:** In Mozquizto, answer question 5.\n",
    "\n",
    "**Important:** A common error is that one forgets to add the term $\\hat{F}_k(z) \\hat{x}_{t+k|t}$ when forming the prediction $\\hat{y}_{t+k | t}$. Note that it is *only* in cases when the input cannot be predicted, i.e., when $x_t$ is a white process, that one omits the $\\hat{F}_k(z) \\hat{x}_{t+k|t}$ term from the prediction. Otherwise, when $x_t$ has any form of structure, it may be predicted, and then the term *should* be included. (To avoid making this error, it is recommended that you *always* include the term; when predicting $\\hat{x}_{t+k|t}$ in the (rare) white noise case, this will of course be zero, so you will just add a zero sequence, which will not corrupt your results, but then you will not forget to add it, which will certainly cause problematic results (this typically appears as predictions that seem to have the correct pattern, but with a too low amplitude).)\n",
    "\n",
    "**Important:** Another common error is that one removes a different number of initial samples when creating $\\frac{\\hat{G}_k(z)}{C(z)} x_t$ and $\\hat{F}_k(z) \\hat{x}_{t+k|t}$; as discussed before, one needs to remove the same number of samples as the order of the denominator polynomial to avoid the problem of the initialization of the filter. However, to avoid the sequences to get out of sync with each other, one should remove the *same* number of samples from both sequences, so that one removes the maximum of the number of samples that are required to be removed from either sequence.\n",
    "\n",
    "**Task:** Using $k=3$, what is the variance of the prediction errors? Plot the process, the prediction and the prediction errors.\n",
    "\n",
    "A common error when making predictions of ARMAX and BJ processes is to forget to add the $\\hat{F}_k(z) \\hat{x}_{t+k|t}$ term. Plot this erroneous prediction and the corresponding prediction errors. Can you see how this error appears in your prediction?\n",
    "\n",
    "**Be prepared to answer these questions when discussing with the examiner at the computer exercise!**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0d7572ad",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-11-16T12:33:54.647758Z",
     "iopub.status.busy": "2025-11-16T12:33:54.647758Z",
     "iopub.status.idle": "2025-11-16T12:33:54.665816Z",
     "shell.execute_reply": "2025-11-16T12:33:54.665266Z"
    }
   },
   "outputs": [],
   "source": [
    "# Load sturup  data\n",
    "mat_sturup = scipy.io.loadmat(os.path.join(DATA_DIR, 'sturup.mat'))\n",
    "sturup = mat_sturup['sturup'].flatten()\n",
    "\n",
    "\n",
    "# Create a model the data and do predictions\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d73b5e6e",
   "metadata": {},
   "source": [
    "### 2.4 (Optional) Examine the project data\n",
    "\n",
    "Examining the project data, proceed to build a model for the input signal (you will need to do this for each of the inputs you wish to use). Do you need to use a transform of the data? Is the resulting model residual white? Pre-whiten the input and output and form the CCF. What seems to be a suitable model? Plot the output as compared to the filtered input - are you explaining a significant part of the output? Estimate the resulting BJ model - is the model residual (reasonably) white?\n",
    "\n",
    "Often, these steps take quite some time - and sometimes one is better off just guessing suitable model orders for $B(z)$ and $A_2(z)$... If it seems problematic to use the above scheme, try with a simple model, using only $B(z) = b_0$ and vary the delay to see what seems to work - then perhaps add a $b_1$ term? Perhaps try some other term? Maybe you can get better results by adding a simple $A_2(z)$ polynomial? Can you remove some coefficients? Be careful to add many parameters here, these polynomials should likely be small.\n",
    "\n",
    "Having now formed a decent model, try to form a one-step prediction using your model. Plot the predicted signal as compared to the output and the naive predictor. Does your model seem to work? Is the prediction residual white? Compare the residual variance for your predictor to that of the naive predictor; did you manage to beat the naive predictor?\n",
    "\n",
    "**Hint:** The above steps will typically form key steps in the project, so the time you spend on this now will be time saved later on..."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
